\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}      % environment for algorithms
\usepackage{algorithmic}    % pseudocode


\title{RL-Musician: A Tool for Music Composition with Deep Reinforcement Learning}

\date{September 30, 2019}

\author{
  Nikolay~Lysenko\\
  \texttt{nikolay.lysenko.1992@gmail.com} \\
}

\renewcommand{\headeright}{A draft}
\renewcommand{\undertitle}{A draft}

\begin{document}
\maketitle

\begin{abstract}
The notion of creativity is wider than generating something inspired by reference pieces of art. It also includes generation of something that meets criteria of being an art, but is not backed by existing pieces. On the one hand, supervised machine learning is limited by the former scope, because it requires a dataset. Reinforcement learning can be applied instead if the goal is to reveal more aspects of creativity. On the other hand, numerous recent advances in music generation are achieved with supervised deep learning. In this paper, a method that uses both reinforcement learning and deep learning is developed for automated music composition. An agent with neural network as actor model is trained to compose musical pieces by interacting with a piano roll environment. The environment scores submitted pieces based on some hand-written evaluational rules derived from music theory. No dataset is involved, because cross-entropy method is used instead of gradient-based methods.
\end{abstract}

\keywords{algorithmc composition \and music generation \and reinforcement learning}


\section{Introduction}
\label{sec:introduction}

Algorithmic music composition is automatic generation of outputs representing musical pieces and written in some formal notation. To name a few of common notations, there are sheet music, tablature, and MIDI standard. It is not required from output representation to unambiguously define sound waveform. For example, sheet music leaves exact loudness of played notes to discretion of a performer and may include only imprecise hints like pianissimo (very quiet). Anyway, there are parameters of sounds that must be determined by their representation. Usually, such parameters are pitch, start time, and duration.

Currently, there are no perfect tools for algorithmic composition. Plenty of various approaches for composing music automatically exist, but none of them produce well-structured and novel pieces that can not be distinguished from works of a talented human composer. Thus, there is an open research problem. Several recent breakthroughs in it are accomplished with machine learning and both supervised learning \cite{johnson2017generating, payne2019musenet} and reinforcement learning \cite{smith2012reinforcement} are applicable. Moreover, some researchers combine them \cite{jaques2016generating, kotecha2018bach, kumar2019polyphonic}.

In this paper, a new approach to music composition is suggested. Although it is quite straightforward, to the best of the author's knowledge it is not described anywhere, so the first contribution is its rigorous definition. The second contribution\footnote{As of now, it is in progress.} is proper setup and tuning of the parameters. It is often the case that accurately tuned simple methods outperform complex methods \cite{dacrema2019are} and, in addition, they are more transparent and less demanding.

Brief outline of the approach is as follows. There is an environment with 2D table representing piano roll which means that rows of the table correspond to consecutive notes and columns of the table correspond to time steps. An agent observes current time step and some previous time steps. An action of the agent can be either playing a note at the current time step or moving to the next time step. When the last time step of the piano roll is reached, episode is ended and the agent receives reward based on evaluation of created composition. Trainable parameters of the agent are weights of a neural network used as a so called actor model, i.e., a model that maps observation to probabilities of actions. These parameters are trained with cross-entropy method \cite{rubinstein1997optimization} (alternatively, they can be trained with genetic algorithms, ant colony algorithms, or evolutionary strategies \cite{salimans2017evolution}).

Obviously, the approach belongs to reinforcement learning, because there is an environment, but a dataset is absent. Nevertheless, there are strong connections with supervised deep learning. From generation of new pieces point of view, actor model returns probabilities of next notes given current state. This is similar to sampling new sequences from a next-step prediction model trained with maximum likelihood method on a dataset of existing sequences.

More details on the methodology are provided in Section \ref{sec:methodology}, but here it is appropriate to discuss its advantages. The reasons for not involving supervised training at all are as follows:
\begin{itemize}
	\item Finding new ways of music creation is a more challenging task than imitation of famous pieces. If no known pieces are used, chances are that the harder problem is considered and it is not replaced with the simpler problem of imitation.
	\item There are tuning systems other than equal temperament (for instance, in microtonal music). For some of them it may be impossible to collect dataset large enough to allow training models in a supervised fashion. However, developers of a tuning system should know some underlying principles and so (at least, in theory) it is possible to create evaluational rules and train an agent based on them.
\end{itemize}

Further, sequential generation of composition by neural network implies that any well-studied architectures from sequence generation tasks can be plugged in and tested. The only difference is that now these neural networks can not be trained with variations of gradient descent, but can be trained with cross-entropy method or alternative methods.

Actually, results reported at this draft version are far from using above advantages at full scale. The current study is rather a proof-of-concept, but this proof-of-concept is easily extendable and ideas on how it can be improved are listed in Section \ref{sec:improvements}.


\section{Background and Related Work}
\label{sec:literature}

\subsection{Algorithmic Composition}
\label{subsec:composition}

In algorithmic composition domain, numerous links between reinforcement learning and deep learning are established.

Reinforcement learning can be used for altering weights of recurrent neural networks trained to generate music sequentially \cite{jaques2016generating, kotecha2018bach}. The goal is to make generated pieces more structured and conformed with music theory rules. To define an environment, let its state be composed of recurrent neural networks states and previously played notes, let an action be an output for current time step and let reward depend on both evaluational rules and probability of output according to initial RNN. Rewards are granted immediately after a step and so DQN (Deep Q-Networks \cite{mnih2013playing}) are preferred over cross-entropy method as a training algorithm.

Usually, softmax activation function is used in the last layer of generative RNN. However, it is possible to consider a family of activation functions parametrized by one parameter $t$ such that output distribution can vary from atomic distribution concentrated at the most probable action ($t = 0$) to distribution returned with softmax activation ($t = 1$) and to uniform distribution ($t \to +\infty$). Also, input vector can be extended by introducing an additional part indicating origination of initial input (so called plan). Then an environment can be defined so that state is a pair of $t$ and plan, actions are changes in either $t$ or plan, and reward depends on evaluation of produced with these settings piece \cite{kumar2019polyphonic}.

Since algorithmic composition can be framed as training of a generative model, it sounds natural to try one of the most salient examples of generative models -- generative adversarial networks (GAN) \cite{goodfellow2014gan}. However, widespread notations for music assume sequences of discrete values but classical GAN work well only with continuous data, because gradient of discrete-valued functions is uninformative. Techniques originating from RL can be used to overcome this obstacle \cite{yu2016seqgan,hjelm2017boundary}. Namely, generator is trained with policy gradient method \cite{williams1992simple}. Such methodology is applied to various tasks and music composition is amongst them \cite{limaguimaraes2017objective}.

Nevertheless, not all methods for automated music generation use deep learning. Musical pieces itself can be generated with genetic algorithms \cite{felice2002genorchestra} where selection is based on scores returned by hand-written evaluation rules.

\subsection{Cross-Entropy Method}
\label{subsec:crossentropy}

Initially, cross-entropy method was developed for estimation of rare events probability \cite{rubinstein1997optimization}. However, it was found that it is also appropriate for solving optimization problems. More detailed discussion of cross-entropy method can be found in \cite{boer2005tutorial}.

\begin{algorithm}
	\caption{Cross-entropy method for optimization} \label{alg:crossentropy}
	\textbf{Input:} $X$ -- set of elements, $f: X \to \mathbb{R}$ -- target function, $u(\cdot, w)$ -- probabilistic distribution over $X$ parametrized by vector $w$. \\
	\textbf{Output:} $\hat{w}$ -- approximate solution to the problem $\max_w \mathbb{E}_{x \sim u(\cdot, w)} f(x)$. \\
	\textbf{Hyperparameters:} $w_0$ -- initial value of $w$; $N$ -- number of iterations, $n$ -- number of vectors to draw at each iteration, $\sigma$ -- standard deviation for vectors generation, $m$ -- number of trials for each vector; $\rho$ -- fraction of best vectors to use for update; $\alpha$ -- smoothing coefficient of updates.
	\begin{algorithmic}[1]
		\FORALL{$i \in \{1, \dots, N\}$}
		    \FORALL{$j \in \{1, \dots, n\}$}
		        \STATE{draw $w_{ij} \sim \mathcal{N}(\cdot \vert w_{i-1}, \sigma)$}
		        \STATE{$r_j \gets \sum_{k = 1}^{m} f(x_k)$ where $x_k \sim u(\cdot, w_{ij})$}
		    \ENDFOR
		    \STATE{$r_{\mathrm{threshold}} \gets$ $[\rho n]$-th highest value of $\{r_j: j \in \{1, \dots, n\}\}$}
		    \STATE{$J \gets \{j: r_j \ge r_{\mathrm{threshold}}\}$}
		    \STATE{$w_i \gets \alpha w_{i-1} + (1 - \alpha)(\sum_{j \in J} w_{ij}) / [\rho n]$}
		\ENDFOR
	    \STATE{$\hat{w} \gets w_N$}
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:crossentropy} defines a variant of cross-entropy method for optimization. Sometimes, hyperparameter $m$ is omitted and intermediate results are not aggregated over multiple trials. In case of $u(\cdot, w)$ that acts like a deterministic function of $w$, $m$ is redundant, but, in general case, terminal result can be improved by setting $m > 1$.


\section{Methodology}
\label{sec:methodology}

\subsection{Setup}
\label{subsec:setup}

For this study, piano roll is chosen as music representation format, but there are some subtleties.

Here, each row of a table does not correspond to a particular note or pitch. Relative axis is used instead of absolute axis. All that is important is that each row is one semitone higher than the row right below it. Thus, a musical piece is given only up to transposition and user-defined value for a pivot note is required to define absolute pitches.

\subsection{Evaluational Rules}
\label{subsec:setup}


\section{Experimental Results}
\label{sec:results}

A software implementation of the above methodology in Python programming language is available on GitHub\footnote{\url{https://github.com/Nikolay-Lysenko/rl-musician}}. The code has built-in documentation, is covered with unit tests, and is released as a package on PyPI\footnote{\url{https://pypi.org/project/rl-musician/0.1.1/}}.

The implementation relies on some open-source tools \cite{brockman2016openai,chollet2015keras,oliphant2006guide,raffel2014intuitive,dong2018pypianoroll}.


\section{Further Improvements}
\label{sec:improvements}
% better evaluational rules (tonality, rhythm);
% randomized initial observations;
% LSTM as actor model;
% evolutionary strategies instead of cross-entropy;
% start from supervisedly pre-trained weigths;


\section{Conclusion}
\label{sec:conclusion}


\bibliographystyle{unsrt}  
\bibliography{references}

\end{document}
